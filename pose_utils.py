import cv2
import numpy as np
from ultralytics import YOLO

# ================================
# üéØ C·∫§U H√åNH MODEL YOLOv8-Pose
# ================================
# model nh·∫π, ph√π h·ª£p demo ho·∫∑c CPU
YOLO_MODEL = YOLO("yolov8n-pose.pt")

# ================================
# üßç‚Äç‚ôÄÔ∏è Tr√≠ch xu·∫•t keypoints nhi·ªÅu ng∆∞·ªùi
# ================================
def extract_multi_person_keypoints(video_path, max_people=5):
    """
    Tr√≠ch xu·∫•t pose keypoints t·ª´ video c√≥ nhi·ªÅu ng∆∞·ªùi m√∫a.
    Tr·∫£ v·ªÅ danh s√°ch m·∫£ng numpy [person_1, person_2, ...]
    """
    cap = cv2.VideoCapture(video_path)
    people_sequences = [[] for _ in range(max_people)]

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        results = YOLO_MODEL(frame, verbose=False)
        if len(results) == 0:
            continue

        keypoints_all = results[0].keypoints
        if keypoints_all is None:
            continue

        poses = keypoints_all.xy.cpu().numpy()  # (N, 17, 2)
        n_people = min(len(poses), max_people)

        for i in range(n_people):
            coords = poses[i].flatten()
            people_sequences[i].append(coords)

    cap.release()
    # ch·ªâ tr·∫£ v·ªÅ nh·ªØng ng∆∞·ªùi c√≥ d·ªØ li·ªáu
    return [np.array(seq) for seq in people_sequences if len(seq) > 0]


# ================================
# üßÆ Trung b√¨nh khung x∆∞∆°ng nh√≥m
# ================================
def average_group_pose(people_sequences):
    """T√≠nh trung b√¨nh khung x∆∞∆°ng c·ªßa nh√≥m."""
    if not people_sequences:
        return np.zeros((1, 34))  # 17 ƒëi·ªÉm * 2 t·ªça ƒë·ªô
    min_len = min(len(seq) for seq in people_sequences)
    trimmed = [seq[:min_len] for seq in people_sequences]
    return np.mean(trimmed, axis=0)


# ================================
# üé• Hi·ªÉn th·ªã skeleton + ƒëi·ªÉm t·ª´ng ng∆∞·ªùi
# ================================
def overlay_skeleton_with_scores(video_path, output_path="temp_overlay.mp4", scores=None):
    """
    Hi·ªÉn th·ªã khung x∆∞∆°ng (pose skeleton) v√† ƒëi·ªÉm t·ª´ng ng∆∞·ªùi tr√™n video.
    H·ªó tr·ª£ multi-person t·ª´ YOLOv8-Pose.
    """
    cap = cv2.VideoCapture(video_path)
    width, height = int(cap.get(3)), int(cap.get(4))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

    COLORS = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255)
    ]

    # ‚úÖ Danh s√°ch k·∫øt n·ªëi kh·ªõp (17 keypoints theo YOLOv8)
    # M·ªói tuple l√† c·∫∑p ch·ªâ s·ªë hai ƒëi·ªÉm c·∫ßn n·ªëi
    SKELETON_CONNECTIONS = [
        (5, 7), (7, 9),    # tay ph·∫£i
        (6, 8), (8, 10),   # tay tr√°i
        (5, 6),            # vai n·ªëi nhau
        (11, 12),          # h√¥ng n·ªëi nhau
        (5, 11), (6, 12),  # th√¢n
        (11, 13), (13, 15),  # ch√¢n ph·∫£i
        (12, 14), (14, 16)   # ch√¢n tr√°i
    ]

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        results = YOLO_MODEL(frame, verbose=False)
        frame_vis = frame.copy()

        if len(results) > 0 and results[0].keypoints is not None:
            poses = results[0].keypoints.xy.cpu().numpy()  # (N, 17, 2)
            for i, pts in enumerate(poses):
                pts = pts.astype(int)
                color = COLORS[i % len(COLORS)]

                # V·∫Ω ƒë∆∞·ªùng n·ªëi gi·ªØa c√°c kh·ªõp (connections)
                for a, b in SKELETON_CONNECTIONS:
                    if a < len(pts) and b < len(pts):
                        xa, ya = pts[a]
                        xb, yb = pts[b]
                        cv2.line(frame_vis, (xa, ya), (xb, yb), color, 2)

                # V·∫Ω ƒëi·ªÉm kh·ªõp
                for (x, y) in pts:
                    cv2.circle(frame_vis, (x, y), 3, color, -1)

                # T√≠nh trung t√¢m ƒë·ªÉ hi·ªÉn th·ªã nh√£n
                x_mean, y_mean = np.mean(pts, axis=0).astype(int)

                if scores and i < len(scores):
                    label = f"P {i+1}: {scores[i]:.1f}"
                else:
                    label = f"P {i+1}"

                cv2.putText(frame_vis, label, (x_mean - 40, y_mean - 20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)

        out.write(frame_vis)

    cap.release()
    out.release()
    return output_path

